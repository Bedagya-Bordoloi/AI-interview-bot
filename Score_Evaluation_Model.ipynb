{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8B4Jc3Jd8xQ1J9Q2Qz9nU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yourusername/yourrepo/blob/main/Score_Evaluation_Model_WandB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Score Evaluation Model with W&B Integration"
      ],
      "metadata": {
        "id": "n1C-Jw2AIxwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate pandas scikit-learn matplotlib seaborn wandb --quiet"
      ],
      "metadata": {
        "id": "j8pyPNGbJCYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import wandb\n",
        "import numpy as np\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "kQ_-AxDrUZ0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()\n",
        "\n",
        "wandb.init(project=\"score-evaluation-model\", config={\n",
        "    \"model_type\": \"distilbert-base-uncased\",\n",
        "    \"batch_size\": 8,\n",
        "    \"epochs\": 4,\n",
        "    \"learning_rate\": 2e-5,\n",
        "    \"test_size\": 0.1\n",
        "})"
      ],
      "metadata": {
        "id": "u5OWDFFRT44J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "df = pd.read_csv(list(uploaded.keys())[0]\n",
        "\n",
        "wandb.log({\n",
        "    \"dataset_size\": len(df),\n",
        "    \"score_distribution\": wandb.plot.histogram(wandb.Table(data=df, columns=[\"score\"]), \n",
        "    \"sample_data\": wandb.Table(dataframe=df.head())\n",
        "})\n",
        "\n",
        "def format_row(row):\n",
        "    return f\"[QUESTION] {row['question']} [IDEAL] {row['ideal_answer']} [USER] {row['user_answer']}\"\n",
        "\n",
        "df['input_text'] = df.apply(format_row, axis=1)\n",
        "df['label'] = df['score'].astype(float)\n",
        "\n",
        "dataset = Dataset.from_pandas(df[['input_text', 'label']])\n",
        "dataset = dataset.train_test_split(test_size=wandb.config.test_size, seed=42)"
      ],
      "metadata": {
        "id": "kQ_-AxDrUZ0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(wandb.config.model_type)\n",
        "\n",
        "def tokenize(example):\n",
        "    return tokenizer(example['input_text'], truncation=True, padding=\"max_length\")\n",
        "\n",
        "tokenized = dataset.map(tokenize)"
      ],
      "metadata": {
        "id": "kQ_-AxDrUZ0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    wandb.config.model_type, \n",
        "    num_labels=1\n",
        ")"
      ],
      "metadata": {
        "id": "XjUF1McMU9Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(p):\n",
        "    preds = p.predictions.flatten()\n",
        "    labels = p.label_ids\n",
        "    \n",
        "    mse = mean_squared_error(labels, preds)\n",
        "    rmse = np.sqrt(mse)\n",
        "    \n",
        "    wandb.log({\"eval_mse\": mse, \"eval_rmse\": rmse})\n",
        "    \n",
        "    return {\"mse\": mse, \"rmse\": rmse}\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"score_model\",\n",
        "    per_device_train_batch_size=wandb.config.batch_size,\n",
        "    per_device_eval_batch_size=wandb.config.batch_size,\n",
        "    num_train_epochs=wandb.config.epochs,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"logs\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    report_to=\"wandb\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "wQvIVFdpVsIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "BM7mD5QJV7UY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer.predict(tokenized[\"test\"])\n",
        "preds = predictions.predictions.flatten()\n",
        "labels = predictions.label_ids\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=labels, y=preds, alpha=0.6)\n",
        "plt.xlabel(\"True Score\")\n",
        "plt.ylabel(\"Predicted Score\")\n",
        "plt.title(\"True vs Predicted Scores on Validation Set\")\n",
        "plt.grid(True)\n",
        "\n",
        "wandb.log({\"true_vs_predicted\": wandb.Image(plt)})\n",
        "plt.show()\n",
        "\n",
        "wandb.log({\n",
        "    \"predictions\": wandb.Table(\n",
        "        columns=[\"True Score\", \"Predicted Score\"],\n",
        "        data=list(zip(labels.tolist(), preds.tolist()))\n",
        "})"
      ],
      "metadata": {
        "id": "6xVb84WTV7uJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"scoring_distilbert_model\")\n",
        "tokenizer.save_pretrained(\"scoring_distilbert_model\")\n",
        "\n",
        "!zip -r scoring_model.zip scoring_distilbert_model\n",
        "\n",
        "artifact = wandb.Artifact('scoring_model', type='model')\n",
        "artifact.add_file(\"scoring_model.zip\")\n",
        "wandb.log_artifact(artifact)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "8xVb84WTV7uJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
