{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yourusername/yourrepo/blob/main/AI_Interviewer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Interview Question Answer Dataset Creation\n",
        "\n",
        "This notebook creates a comprehensive dataset of technical interview questions and answers by:\n",
        "1. Scraping questions from multiple GitHub repositories\n",
        "2. Extracting Q&A pairs from technical articles\n",
        "3. Cleaning and merging the collected data\n",
        "4. Adding role-based tagging for better organization"
      ],
      "metadata": {
        "id": "project-overview"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation of Required Libraries"
      ],
      "metadata": {
        "id": "install-libraries"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas requests beautifulsoup4 scikit-learn numpy lxml"
      ],
      "metadata": {
        "id": "install-commands"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Collection: Scraping Q&A from GitHub Repositories\n",
        "\n",
        "This section extracts interview questions from GitHub README files by:\n",
        "1. Identifying relevant repositories\n",
        "2. Processing their README files\n",
        "3. Extracting all interview-related URLs"
      ],
      "metadata": {
        "id": "github-scraping"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "\n",
        "# List of GitHub raw markdown URLs containing interview questions\n",
        "github_raw_urls = [\n",
        "    \"https://raw.githubusercontent.com/DopplerHQ/awesome-interview-questions/master/README.md\",\n",
        "    \"https://raw.githubusercontent.com/bregman-arie/devops-interview-questions/master/README.md\",\n",
        "    \"https://raw.githubusercontent.com/darshanjain-ml/Interview-Question-Data/main/README.md\",\n",
        "    \"https://raw.githubusercontent.com/Ebazhanov/linkedin-skill-assessments-quizzes/master/README.md\",\n",
        "    \"https://raw.githubusercontent.com/30-seconds/30-seconds-of-interviews/master/README.md\"\n",
        "]\n",
        "\n",
        "# Pattern to identify interview-related URLs\n",
        "interview_url_pattern = re.compile(\n",
        "    r'https?://[^\\s\\)]+(interview|question|answers)[^\\s\\)]+', re.IGNORECASE)\n",
        "\n",
        "all_links = set()\n",
        "\n",
        "for url in github_raw_urls:\n",
        "    print(f\"üîç Processing: {url}\")\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        content = response.text\n",
        "\n",
        "        # Extract all URLs containing interview/question/answers keywords\n",
        "        for match in re.findall(r'https?://[^\\s\\)\\]]+', content):\n",
        "            if re.search(r\"(interview|question|answers)\", match, re.IGNORECASE):\n",
        "                all_links.add(match.strip().rstrip(\").,\"))\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error reading {url}: {e}\")\n",
        "\n",
        "# Filter to keep only high-quality sources\n",
        "preferred_domains = [\"medium.com\", \"dev.to\", \"geeksforgeeks.org\", \"simplilearn.com\", \n",
        "                    \"codecademy.com\", \"roadmap.sh\", \"w3webschool.com\"]\n",
        "filtered_links = [link for link in all_links if any(domain in link for domain in preferred_domains)]\n",
        "\n",
        "# Save extracted links to file\n",
        "with open(\"github_extracted_qna_urls.txt\", \"w\") as f:\n",
        "    for link in sorted(filtered_links):\n",
        "        f.write(link + \"\\n\")\n",
        "\n",
        "print(f\"‚úÖ Found {len(filtered_links)} filtered Q&A links.\")"
      ],
      "metadata": {
        "id": "github-url-extraction"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q&A Extraction from Web Sources\n",
        "\n",
        "This section scrapes actual question-answer pairs from the collected URLs by:\n",
        "1. Identifying question elements (headings, bold text)\n",
        "2. Extracting the subsequent answer text\n",
        "3. Ensuring minimum quality standards (answer length > 20 chars)"
      ],
      "metadata": {
        "id": "qa-extraction"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "from google.colab import files\n",
        "\n",
        "# Function to scrape Q&A from a single URL\n",
        "def scrape_qna(url):\n",
        "    try:\n",
        "        res = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=10)\n",
        "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to fetch {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "    qna_pairs = []\n",
        "    # Look for questions in headings and bold text\n",
        "    for tag in soup.find_all(['h2', 'h3', 'strong', 'b']):\n",
        "        q = tag.get_text().strip()\n",
        "        # Only consider text with question marks and minimum length\n",
        "        if '?' in q and len(q.split()) >= 3:\n",
        "            a_tag = tag.find_next('p')\n",
        "            if a_tag:\n",
        "                a = a_tag.get_text().strip()\n",
        "                if len(a) >= 20:  # Minimum answer length\n",
        "                    qna_pairs.append({\n",
        "                        \"question\": q,\n",
        "                        \"answer\": a,\n",
        "                        \"source\": url\n",
        "                    })\n",
        "    return qna_pairs\n",
        "\n",
        "# Scrape all collected URLs\n",
        "all_data = []\n",
        "for url in urls:\n",
        "    print(f\"üîç Scraping: {url}\")\n",
        "    data = scrape_qna(url)\n",
        "    all_data.extend(data)\n",
        "    time.sleep(1)  # Be polite with requests\n",
        "\n",
        "# Save results\n",
        "df = pd.DataFrame(all_data)\n",
        "df.to_csv(\"scraped_qna.csv\", index=False)\n",
        "df.to_json(\"scraped_qna.json\", orient=\"records\", indent=2)\n",
        "\n",
        "print(f\"‚úÖ Scraping complete. Total Q&A pairs: {len(all_data)}\")"
      ],
      "metadata": {
        "id": "scraping-implementation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning and Merging\n",
        "\n",
        "This section processes the collected data by:\n",
        "1. Removing duplicate questions\n",
        "2. Filtering short answers\n",
        "3. Cleaning question formatting\n",
        "4. Merging with existing datasets"
      ],
      "metadata": {
        "id": "data-cleaning"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load and clean dataset\n",
        "df = pd.read_csv(\"scraped_qna.csv\")\n",
        "\n",
        "# Remove duplicate questions\n",
        "df = df.drop_duplicates(subset='question')\n",
        "\n",
        "# Remove answers shorter than 20 characters\n",
        "df = df[df['answer'].str.len() > 20]\n",
        "\n",
        "# Clean question prefixes (Q1., 2. etc)\n",
        "df['question'] = df['question'].str.replace(r'^\\s*(Q?\\d+\\.*)\\s*', '', regex=True)\n",
        "\n",
        "# Save cleaned data\n",
        "df.to_csv(\"cleaned_dataset.csv\", index=False)\n",
        "df.to_json(\"cleaned_dataset.json\", orient=\"records\", indent=2)\n",
        "\n",
        "# Merge with existing dataset if available\n",
        "try:\n",
        "    existing_df = pd.read_csv(\"combined_qna_dataset.csv\")\n",
        "    combined = pd.concat([df, existing_df], ignore_index=True)\n",
        "    combined.drop_duplicates(subset=\"question\", inplace=True)\n",
        "    combined.to_csv(\"combined_qna_dataset.csv\", index=False)\n",
        "    print(\"‚úÖ Successfully merged with existing dataset\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ÑπÔ∏è No existing dataset found - using only scraped data\")\n",
        "    df.to_csv(\"combined_qna_dataset.csv\", index=False)"
      ],
      "metadata": {
        "id": "cleaning-code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Role-Based Tagging\n",
        "\n",
        "This section automatically categorizes questions by job role using keyword matching:"
      ],
      "metadata": {
        "id": "role-tagging"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def guess_role(question):\n",
        "    \"\"\"Automatically tag questions with job roles based on keywords\"\"\"\n",
        "    q = str(question).lower()\n",
        "    \n",
        "    role_keywords = {\n",
        "        \"Frontend Developer\": [\"frontend\", \"react\", \"html\", \"css\", \"javascript\"],\n",
        "        \"Backend Developer\": [\"backend\", \"sql\", \"api\", \"database\", \"server\"],\n",
        "        \"Cloud Architect\": [\"cloud\", \"aws\", \"azure\", \"gcp\"],\n",
        "        \"Cybersecurity Analyst\": [\"cyber\", \"security\", \"encryption\", \"firewall\"],\n",
        "        \"DevOps Engineer\": [\"devops\", \"ci/cd\", \"docker\", \"kubernetes\"],\n",
        "        \"AI Engineer\": [\"ai\", \"ml\", \"model\", \"neural\", \"tensorflow\"],\n",
        "        \"UX Developer\": [\"ux\", \"ui\", \"design\", \"user experience\"],\n",
        "        \"Product Manager\": [\"product\", \"roadmap\", \"agile\"],\n",
        "        \"Software Engineer\": [\"python\", \"java\", \"oop\", \"algorithm\"]\n",
        "    }\n",
        "    \n",
        "    for role, keywords in role_keywords.items():\n",
        "        if any(keyword in q for keyword in keywords):\n",
        "            return role\n",
        "    return \"Other\"\n",
        "\n",
        "# Apply role tagging\n",
        "df['role'] = df['question'].apply(guess_role)\n",
        "\n",
        "# Save tagged dataset\n",
        "df.to_csv(\"auto_tagged_dataset.csv\", index=False)\n",
        "print(f\"Role distribution:\\n{df['role'].value_counts()}\")"
      ],
      "metadata": {
        "id": "tagging-implementation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Analysis\n",
        "\n",
        "Basic analysis of the final dataset:"
      ],
      "metadata": {
        "id": "dataset-analysis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load final dataset\n",
        "df = pd.read_csv(\"auto_tagged_dataset.csv\")\n",
        "\n",
        "# Basic info\n",
        "print(\"\\nüîç Dataset Summary:\")\n",
        "print(df.info())\n",
        "\n",
        "# Role distribution\n",
        "print(\"\\nüìä Role Distribution:\")\n",
        "print(df['role'].value_counts())\n",
        "\n",
        "# Sample questions\n",
        "print(\"\\nüìù Sample Questions:\")\n",
        "print(df.sample(5)[['question', 'role']])"
      ],
      "metadata": {
        "id": "analysis-code"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
