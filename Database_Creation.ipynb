{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project-header"
   },
   "source": [
    "# AI Interview Question Answer Dataset\n",
    "\n",
    "This notebook creates a comprehensive dataset of technical interview questions and answers by:\n",
    "- Scraping questions from GitHub repositories\n",
    "- Extracting Q&A pairs from technical articles\n",
    "- Cleaning and merging collected data\n",
    "- Adding role-based tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install-section"
   },
   "source": [
    "## Installation of Required Libraries\n",
    "\n",
    "First, we need to install all necessary Python packages:\n",
    "- `pandas` for data manipulation\n",
    "- `requests` and `beautifulsoup4` for web scraping\n",
    "- Other supporting libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-code"
   },
   "outputs": [],
   "source": [
    "!pip install pandas requests beautifulsoup4 scikit-learn numpy lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "github-scraping-header"
   },
   "source": [
    "## Step 1: Collect GitHub Interview URLs\n",
    "\n",
    "We'll scrape these GitHub repositories for interview questions:\n",
    "1. awesome-interview-questions\n",
    "2. devops-interview-questions\n",
    "3. Interview-Question-Data\n",
    "\n",
    "This code extracts all URLs containing interview questions from their READMEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "github-url-extraction"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "# List of GitHub repositories with interview questions\n",
    "github_raw_urls = [\n",
    "    \"https://raw.githubusercontent.com/DopplerHQ/awesome-interview-questions/master/README.md\",\n",
    "    \"https://raw.githubusercontent.com/bregman-arie/devops-interview-questions/master/README.md\",\n",
    "    \"https://raw.githubusercontent.com/darshanjain-ml/Interview-Question-Data/main/README.md\"\n",
    "]\n",
    "\n",
    "all_links = set()\n",
    "\n",
    "for url in github_raw_urls:\n",
    "    print(f\"Processing: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        content = response.text\n",
    "        \n",
    "        # Extract all interview/question URLs\n",
    "        for match in re.findall(r'https?://[^\\s\\)\\]]+', content):\n",
    "            if re.search(r\"(interview|question|answers)\", match, re.IGNORECASE):\n",
    "                all_links.add(match.strip().rstrip(\").,\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {url}: {e}\")\n",
    "\n",
    "print(f\"Found {len(all_links)} Q&A links\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "filter-urls"
   },
   "source": [
    "## Step 2: Filter High-Quality Sources\n",
    "\n",
    "We'll filter the URLs to keep only reputable technical sites like:\n",
    "- medium.com\n",
    "- dev.to\n",
    "- geeksforgeeks.org\n",
    "\n",
    "This ensures our dataset has reliable content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "filtering-code"
   },
   "outputs": [],
   "source": [
    "preferred_domains = [\"medium.com\", \"dev.to\", \"geeksforgeeks.org\", \"simplilearn.com\"]\n",
    "filtered_links = [link for link in all_links if any(domain in link for domain in preferred_domains)]\n",
    "\n",
    "# Save the filtered links\n",
    "with open(\"qna_urls.txt\", \"w\") as f:\n",
    "    for link in filtered_links:\n",
    "        f.write(link + \"\\n\")\n",
    "\n",
    "print(f\"Kept {len(filtered_links)} high-quality links\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scraping-section"
   },
   "source": [
    "## Step 3: Scrape Q&A Pairs\n",
    "\n",
    "Now we'll scrape actual questions and answers from each URL by:\n",
    "1. Looking for question headings (h2/h3 tags)\n",
    "2. Extracting the next paragraph as the answer\n",
    "3. Ensuring answers meet minimum length requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scraping-code"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def scrape_qna(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        qna = []\n",
    "        # Find all potential question elements\n",
    "        for tag in soup.find_all(['h2', 'h3', 'strong', 'b']):\n",
    "            question = tag.get_text().strip()\n",
    "            if '?' in question:  # Likely a question\n",
    "                answer_tag = tag.find_next('p')\n",
    "                if answer_tag:\n",
    "                    answer = answer_tag.get_text().strip()\n",
    "                    if len(answer) > 20:  # Minimum answer length\n",
    "                        qna.append({\"question\": question, \"answer\": answer, \"source\": url})\n",
    "        return qna\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Scrape all filtered URLs\n",
    "all_qna = []\n",
    "for url in filtered_links:\n",
    "    print(f\"Scraping: {url}\")\n",
    "    all_qna.extend(scrape_qna(url))\n",
    "    time.sleep(1)  # Be polite with requests\n",
    "\n",
    "print(f\"Scraped {len(all_qna)} Q&A pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleaning-section"
   },
   "source": [
    "## Step 4: Clean the Dataset\n",
    "\n",
    "Now we'll clean the data by:\n",
    "1. Removing duplicate questions\n",
    "2. Filtering short answers\n",
    "3. Cleaning question formatting\n",
    "4. Saving to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleaning-code"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_qna)\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates(subset=\"question\")\n",
    "\n",
    "# Remove short answers\n",
    "df = df[df['answer'].str.len() > 20]\n",
    "\n",
    "# Clean question prefixes (Q1., 2. etc)\n",
    "df['question'] = df['question'].str.replace(r'^\\s*(Q?\\d+\\.*)\\s*', '', regex=True)\n",
    "\n",
    "# Save cleaned data\n",
    "df.to_csv(\"interview_questions.csv\", index=False)\n",
    "print(f\"Final dataset has {len(df)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tagging-section"
   },
   "source": [
    "## Step 5: Add Role Tagging\n",
    "\n",
    "We'll automatically categorize questions by job role using keyword matching:\n",
    "- Frontend (React, HTML, CSS)\n",
    "- Backend (SQL, API, Database)\n",
    "- DevOps (Docker, CI/CD)\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tagging-code"
   },
   "outputs": [],
   "source": [
    "def tag_role(question):\n",
    "    \"\"\"Categorize question by job role based on keywords\"\"\"\n",
    "    q = question.lower()\n",
    "    if 'react' in q or 'javascript' in q:\n",
    "        return \"Frontend\"\n",
    "    elif 'sql' in q or 'database' in q:\n",
    "        return \"Backend\"\n",
    "    elif 'docker' in q or 'kubernetes' in q:\n",
    "        return \"DevOps\"\n",
    "    elif 'python' in q or 'java' in q:\n",
    "        return \"Software Engineer\"\n",
    "    else:\n",
    "        return \"General\"\n",
    "\n",
    "# Apply tagging\n",
    "df['role'] = df['question'].apply(tag_role)\n",
    "\n",
    "# Save final dataset\n",
    "df.to_csv(\"tagged_interview_questions.csv\", index=False)\n",
    "print(\"Role distribution:\")\n",
    "print(df['role'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "analysis-section"
   },
   "source": [
    "## Final Dataset Analysis\n",
    "\n",
    "Let's examine our completed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analysis-code"
   },
   "outputs": [],
   "source": [
    "print(\"Dataset summary:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nSample questions:\")\n",
    "print(df.sample(5)[['question', 'role']])"
   ]
  }
 ]
}
